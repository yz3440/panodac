{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"panodac # Metric depth estimation for any camera. Perspective, fisheye, 360\u00b0 panorama. Based on Depth Any Camera (CVPR 2025). Installation # pip install panodac Or install from source: pip install \"panodac @ git+https://github.com/yz3440/panodac.git\" Quick Start # import panodac # Predict depth from any image depth = panodac . predict ( \"photo.jpg\" ) # depth is a numpy array (H, W) with metric depth in meters # Use a specific model depth = panodac . predict ( \"panorama.jpg\" , model = \"outdoor-swinl\" ) # List available models print ( panodac . list_models ()) # ['outdoor-resnet101', 'outdoor-swinl', 'indoor-resnet101', 'indoor-swinl'] Models # Model Use Case Speed Quality outdoor-resnet101 Outdoor Fast Good outdoor-swinl Outdoor Slow Best indoor-resnet101 Indoor Fast Good indoor-swinl Indoor Slow Best Models auto-download from HuggingFace on first use (~500MB each). Device Selection # panodac automatically uses the best available device: Apple Silicon (MPS) \u2014 Used by default on M1/M2/M3 Macs CUDA \u2014 Used when NVIDIA GPU is available CPU \u2014 Fallback when no GPU is available # Check current device print ( panodac . get_device ()) # 'mps', 'cuda', or 'cpu' # Force specific device depth = panodac . predict ( \"image.jpg\" , device = \"cpu\" ) Input Formats # The predict() function accepts multiple input types: import panodac from PIL import Image import numpy as np # File path (string or Path) depth = panodac . predict ( \"photo.jpg\" ) # PIL Image img = Image . open ( \"photo.jpg\" ) depth = panodac . predict ( img ) # NumPy array (H, W, 3) RGB img_np = np . array ( Image . open ( \"photo.jpg\" )) depth = panodac . predict ( img_np ) Panorama Detection # 360\u00b0 equirectangular panoramas (2:1 aspect ratio) are automatically detected and processed with appropriate spherical coordinates. # Panoramas are auto-detected depth = panodac . predict ( \"panorama_360.jpg\" ) Next # Examples - Working scripts API Reference - Full documentation","title":"Home"},{"location":"#panodac","text":"Metric depth estimation for any camera. Perspective, fisheye, 360\u00b0 panorama. Based on Depth Any Camera (CVPR 2025).","title":"panodac"},{"location":"#installation","text":"pip install panodac Or install from source: pip install \"panodac @ git+https://github.com/yz3440/panodac.git\"","title":"Installation"},{"location":"#quick-start","text":"import panodac # Predict depth from any image depth = panodac . predict ( \"photo.jpg\" ) # depth is a numpy array (H, W) with metric depth in meters # Use a specific model depth = panodac . predict ( \"panorama.jpg\" , model = \"outdoor-swinl\" ) # List available models print ( panodac . list_models ()) # ['outdoor-resnet101', 'outdoor-swinl', 'indoor-resnet101', 'indoor-swinl']","title":"Quick Start"},{"location":"#models","text":"Model Use Case Speed Quality outdoor-resnet101 Outdoor Fast Good outdoor-swinl Outdoor Slow Best indoor-resnet101 Indoor Fast Good indoor-swinl Indoor Slow Best Models auto-download from HuggingFace on first use (~500MB each).","title":"Models"},{"location":"#device-selection","text":"panodac automatically uses the best available device: Apple Silicon (MPS) \u2014 Used by default on M1/M2/M3 Macs CUDA \u2014 Used when NVIDIA GPU is available CPU \u2014 Fallback when no GPU is available # Check current device print ( panodac . get_device ()) # 'mps', 'cuda', or 'cpu' # Force specific device depth = panodac . predict ( \"image.jpg\" , device = \"cpu\" )","title":"Device Selection"},{"location":"#input-formats","text":"The predict() function accepts multiple input types: import panodac from PIL import Image import numpy as np # File path (string or Path) depth = panodac . predict ( \"photo.jpg\" ) # PIL Image img = Image . open ( \"photo.jpg\" ) depth = panodac . predict ( img ) # NumPy array (H, W, 3) RGB img_np = np . array ( Image . open ( \"photo.jpg\" )) depth = panodac . predict ( img_np )","title":"Input Formats"},{"location":"#panorama-detection","text":"360\u00b0 equirectangular panoramas (2:1 aspect ratio) are automatically detected and processed with appropriate spherical coordinates. # Panoramas are auto-detected depth = panodac . predict ( \"panorama_360.jpg\" )","title":"Panorama Detection"},{"location":"#next","text":"Examples - Working scripts API Reference - Full documentation","title":"Next"},{"location":"examples/","text":"Examples # Working scripts in examples/ . basic_depth.py # Basic depth prediction and visualization. #!/usr/bin/env python3 \"\"\" Basic depth prediction example. This demonstrates the simplest usage of panodac: predicting depth from a single image. \"\"\" import panodac import numpy as np from PIL import Image import matplotlib.pyplot as plt def main (): # Predict depth - it's this simple! depth = panodac . predict ( \"path/to/your/image.jpg\" ) # depth is a numpy array (H, W) with metric depth in meters print ( f \"Depth shape: { depth . shape } \" ) print ( f \"Depth range: { depth . min () : .2f } m - { depth . max () : .2f } m\" ) # Visualize the depth map plt . figure ( figsize = ( 12 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . title ( \"Input Image\" ) plt . imshow ( Image . open ( \"path/to/your/image.jpg\" )) plt . axis ( \"off\" ) plt . subplot ( 1 , 2 , 2 ) plt . title ( \"Predicted Depth\" ) plt . imshow ( depth , cmap = \"turbo\" ) plt . colorbar ( label = \"Depth (m)\" ) plt . axis ( \"off\" ) plt . tight_layout () plt . savefig ( \"depth_result.jpg\" ) print ( \"Saved visualization to depth_result.jpg\" ) def example_with_options (): \"\"\"Example showing all available options.\"\"\" # List available models print ( \"Available models:\" , panodac . list_models ()) # Output: ['outdoor-resnet101', 'outdoor-swinl', 'indoor-resnet101', 'indoor-swinl'] # Check current device print ( \"Using device:\" , panodac . get_device ()) # Output: cpu, cuda, or mps (depending on your system) # Predict with a specific model depth = panodac . predict ( \"photo.jpg\" , model = \"outdoor-swinl\" , # Higher quality, slower device = \"mps\" , # Force specific device ) # The predict function accepts multiple input types: # 1. File path (string or Path) depth = panodac . predict ( \"photo.jpg\" ) # 2. PIL Image from PIL import Image img = Image . open ( \"photo.jpg\" ) depth = panodac . predict ( img ) # 3. NumPy array (H, W, 3) RGB img_np = np . array ( Image . open ( \"photo.jpg\" )) depth = panodac . predict ( img_np ) if __name__ == \"__main__\" : main () panorama_depth.py # 360\u00b0 panorama depth prediction with point cloud export. #!/usr/bin/env python3 \"\"\" Panorama depth prediction with point cloud export. This example shows how to process 360\u00b0 equirectangular panoramas and export the result as a 3D point cloud. \"\"\" import numpy as np import panodac from PIL import Image def main (): # Path to your panorama image (should have 2:1 aspect ratio) pano_path = \"../assets/test-pano.jpg\" # Predict depth print ( \"Predicting depth...\" ) depth = panodac . predict ( pano_path , model = \"outdoor-resnet101\" ) print ( f \"Depth shape: { depth . shape } \" ) print ( f \"Depth range: { depth . min () : .2f } m - { depth . max () : .2f } m\" ) # Save depth visualization save_depth_visualization ( depth , \"panorama_depth.jpg\" ) # Export as point cloud print ( \"Generating point cloud...\" ) pano_img = np . array ( Image . open ( pano_path )) points , colors = erp_to_pointcloud ( pano_img , depth ) save_ply ( \"panorama_pointcloud.ply\" , points , colors ) print ( f \"Saved point cloud with { len ( points ) } points\" ) def save_depth_visualization ( depth : np . ndarray , output_path : str ): \"\"\"Save a colorized depth visualization.\"\"\" import cv2 # Normalize depth to 0-255 for visualization depth_norm = ( depth - depth . min ()) / ( depth . max () - depth . min ()) depth_vis = ( depth_norm * 255 ) . astype ( np . uint8 ) # Apply colormap depth_colored = cv2 . applyColorMap ( depth_vis , cv2 . COLORMAP_TURBO ) depth_colored = cv2 . cvtColor ( depth_colored , cv2 . COLOR_BGR2RGB ) Image . fromarray ( depth_colored ) . save ( output_path ) print ( f \"Saved depth visualization to { output_path } \" ) def erp_to_pointcloud ( image : np . ndarray , depth : np . ndarray , max_points : int = 500000 , ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Convert equirectangular panorama with depth to 3D point cloud. Args: image: RGB image (H, W, 3) depth: Depth map (H, W) in meters max_points: Maximum number of points to generate Returns: points: (N, 3) xyz coordinates colors: (N, 3) RGB colors (0-255) \"\"\" H , W = depth . shape # Create coordinate grids u = np . linspace ( 0 , 1 , W , dtype = np . float32 ) v = np . linspace ( 0 , 1 , H , dtype = np . float32 ) u , v = np . meshgrid ( u , v ) # Convert to spherical coordinates # u: 0->1 maps to longitude -\u03c0->\u03c0 # v: 0->1 maps to latitude \u03c0/2->-\u03c0/2 longitude = ( u - 0.5 ) * 2 * np . pi latitude = ( 0.5 - v ) * np . pi # Convert to Cartesian coordinates x = depth * np . cos ( latitude ) * np . sin ( longitude ) y = depth * np . sin ( latitude ) z = depth * np . cos ( latitude ) * np . cos ( longitude ) # Flatten and stack points = np . stack ([ x . ravel (), y . ravel (), z . ravel ()], axis = 1 ) colors = image . reshape ( - 1 , 3 ) # Subsample if too many points if len ( points ) > max_points : idx = np . random . choice ( len ( points ), max_points , replace = False ) points = points [ idx ] colors = colors [ idx ] # Filter out invalid points (very far or very close) valid = ( points [:, 2 ] > 0.1 ) & ( points [:, 2 ] < 100 ) points = points [ valid ] colors = colors [ valid ] return points , colors def save_ply ( filename : str , points : np . ndarray , colors : np . ndarray ): \"\"\"Save point cloud to PLY file.\"\"\" with open ( filename , 'w' ) as f : f . write ( \"ply \\n \" ) f . write ( \"format ascii 1.0 \\n \" ) f . write ( f \"element vertex { len ( points ) } \\n \" ) f . write ( \"property float x \\n \" ) f . write ( \"property float y \\n \" ) f . write ( \"property float z \\n \" ) f . write ( \"property uchar red \\n \" ) f . write ( \"property uchar green \\n \" ) f . write ( \"property uchar blue \\n \" ) f . write ( \"end_header \\n \" ) for ( x , y , z ), ( r , g , b ) in zip ( points , colors ): f . write ( f \" { x : .4f } { y : .4f } { z : .4f } { int ( r ) } { int ( g ) } { int ( b ) } \\n \" ) if __name__ == \"__main__\" : main () batch_inference.py # Batch process multiple images from the command line. #!/usr/bin/env python3 \"\"\" Batch processing example. Process multiple images from a directory and save depth maps. \"\"\" import argparse from pathlib import Path import cv2 import numpy as np from PIL import Image from tqdm import tqdm import panodac def main (): parser = argparse . ArgumentParser ( description = \"Batch depth prediction\" ) parser . add_argument ( \"input_dir\" , type = str , help = \"Directory with input images\" ) parser . add_argument ( \"output_dir\" , type = str , help = \"Directory for output depth maps\" ) parser . add_argument ( \"--model\" , type = str , default = \"outdoor-resnet101\" , choices = panodac . list_models (), help = \"Model to use\" ) parser . add_argument ( \"--format\" , type = str , default = \"png\" , choices = [ \"png\" , \"npy\" , \"exr\" ], help = \"Output format for depth maps\" ) args = parser . parse_args () # Create output directory input_dir = Path ( args . input_dir ) output_dir = Path ( args . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) # Find all images extensions = { \".jpg\" , \".jpeg\" , \".png\" , \".webp\" , \".bmp\" } image_files = [ f for f in input_dir . iterdir () if f . suffix . lower () in extensions ] if not image_files : print ( f \"No images found in { input_dir } \" ) return print ( f \"Found { len ( image_files ) } images\" ) print ( f \"Using model: { args . model } \" ) print ( f \"Using device: { panodac . get_device () } \" ) # Process images for img_path in tqdm ( image_files , desc = \"Processing\" ): try : # Predict depth depth = panodac . predict ( str ( img_path ), model = args . model ) # Save output output_name = img_path . stem + \"_depth\" if args . format == \"png\" : # Save as 16-bit PNG (depth in mm) depth_mm = ( depth * 1000 ) . astype ( np . uint16 ) output_path = output_dir / f \" { output_name } .png\" cv2 . imwrite ( str ( output_path ), depth_mm ) elif args . format == \"npy\" : # Save as numpy array (original float values in meters) output_path = output_dir / f \" { output_name } .npy\" np . save ( output_path , depth ) elif args . format == \"exr\" : # Save as OpenEXR (requires opencv-python-headless[contrib]) output_path = output_dir / f \" { output_name } .exr\" cv2 . imwrite ( str ( output_path ), depth . astype ( np . float32 )) # Also save a visualization save_visualization ( depth , output_dir / f \" { output_name } _vis.jpg\" ) except Exception as e : print ( f \" \\n Error processing { img_path . name } : { e } \" ) print ( f \" \\n Results saved to { output_dir } \" ) def save_visualization ( depth : np . ndarray , output_path : Path ): \"\"\"Save a colorized depth visualization.\"\"\" # Normalize depth depth_norm = ( depth - depth . min ()) / ( depth . max () - depth . min () + 1e-8 ) depth_vis = ( depth_norm * 255 ) . astype ( np . uint8 ) # Apply colormap depth_colored = cv2 . applyColorMap ( depth_vis , cv2 . COLORMAP_TURBO ) cv2 . imwrite ( str ( output_path ), depth_colored ) if __name__ == \"__main__\" : main ()","title":"Examples"},{"location":"examples/#examples","text":"Working scripts in examples/ .","title":"Examples"},{"location":"examples/#basic_depthpy","text":"Basic depth prediction and visualization. #!/usr/bin/env python3 \"\"\" Basic depth prediction example. This demonstrates the simplest usage of panodac: predicting depth from a single image. \"\"\" import panodac import numpy as np from PIL import Image import matplotlib.pyplot as plt def main (): # Predict depth - it's this simple! depth = panodac . predict ( \"path/to/your/image.jpg\" ) # depth is a numpy array (H, W) with metric depth in meters print ( f \"Depth shape: { depth . shape } \" ) print ( f \"Depth range: { depth . min () : .2f } m - { depth . max () : .2f } m\" ) # Visualize the depth map plt . figure ( figsize = ( 12 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . title ( \"Input Image\" ) plt . imshow ( Image . open ( \"path/to/your/image.jpg\" )) plt . axis ( \"off\" ) plt . subplot ( 1 , 2 , 2 ) plt . title ( \"Predicted Depth\" ) plt . imshow ( depth , cmap = \"turbo\" ) plt . colorbar ( label = \"Depth (m)\" ) plt . axis ( \"off\" ) plt . tight_layout () plt . savefig ( \"depth_result.jpg\" ) print ( \"Saved visualization to depth_result.jpg\" ) def example_with_options (): \"\"\"Example showing all available options.\"\"\" # List available models print ( \"Available models:\" , panodac . list_models ()) # Output: ['outdoor-resnet101', 'outdoor-swinl', 'indoor-resnet101', 'indoor-swinl'] # Check current device print ( \"Using device:\" , panodac . get_device ()) # Output: cpu, cuda, or mps (depending on your system) # Predict with a specific model depth = panodac . predict ( \"photo.jpg\" , model = \"outdoor-swinl\" , # Higher quality, slower device = \"mps\" , # Force specific device ) # The predict function accepts multiple input types: # 1. File path (string or Path) depth = panodac . predict ( \"photo.jpg\" ) # 2. PIL Image from PIL import Image img = Image . open ( \"photo.jpg\" ) depth = panodac . predict ( img ) # 3. NumPy array (H, W, 3) RGB img_np = np . array ( Image . open ( \"photo.jpg\" )) depth = panodac . predict ( img_np ) if __name__ == \"__main__\" : main ()","title":"basic_depth.py"},{"location":"examples/#panorama_depthpy","text":"360\u00b0 panorama depth prediction with point cloud export. #!/usr/bin/env python3 \"\"\" Panorama depth prediction with point cloud export. This example shows how to process 360\u00b0 equirectangular panoramas and export the result as a 3D point cloud. \"\"\" import numpy as np import panodac from PIL import Image def main (): # Path to your panorama image (should have 2:1 aspect ratio) pano_path = \"../assets/test-pano.jpg\" # Predict depth print ( \"Predicting depth...\" ) depth = panodac . predict ( pano_path , model = \"outdoor-resnet101\" ) print ( f \"Depth shape: { depth . shape } \" ) print ( f \"Depth range: { depth . min () : .2f } m - { depth . max () : .2f } m\" ) # Save depth visualization save_depth_visualization ( depth , \"panorama_depth.jpg\" ) # Export as point cloud print ( \"Generating point cloud...\" ) pano_img = np . array ( Image . open ( pano_path )) points , colors = erp_to_pointcloud ( pano_img , depth ) save_ply ( \"panorama_pointcloud.ply\" , points , colors ) print ( f \"Saved point cloud with { len ( points ) } points\" ) def save_depth_visualization ( depth : np . ndarray , output_path : str ): \"\"\"Save a colorized depth visualization.\"\"\" import cv2 # Normalize depth to 0-255 for visualization depth_norm = ( depth - depth . min ()) / ( depth . max () - depth . min ()) depth_vis = ( depth_norm * 255 ) . astype ( np . uint8 ) # Apply colormap depth_colored = cv2 . applyColorMap ( depth_vis , cv2 . COLORMAP_TURBO ) depth_colored = cv2 . cvtColor ( depth_colored , cv2 . COLOR_BGR2RGB ) Image . fromarray ( depth_colored ) . save ( output_path ) print ( f \"Saved depth visualization to { output_path } \" ) def erp_to_pointcloud ( image : np . ndarray , depth : np . ndarray , max_points : int = 500000 , ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Convert equirectangular panorama with depth to 3D point cloud. Args: image: RGB image (H, W, 3) depth: Depth map (H, W) in meters max_points: Maximum number of points to generate Returns: points: (N, 3) xyz coordinates colors: (N, 3) RGB colors (0-255) \"\"\" H , W = depth . shape # Create coordinate grids u = np . linspace ( 0 , 1 , W , dtype = np . float32 ) v = np . linspace ( 0 , 1 , H , dtype = np . float32 ) u , v = np . meshgrid ( u , v ) # Convert to spherical coordinates # u: 0->1 maps to longitude -\u03c0->\u03c0 # v: 0->1 maps to latitude \u03c0/2->-\u03c0/2 longitude = ( u - 0.5 ) * 2 * np . pi latitude = ( 0.5 - v ) * np . pi # Convert to Cartesian coordinates x = depth * np . cos ( latitude ) * np . sin ( longitude ) y = depth * np . sin ( latitude ) z = depth * np . cos ( latitude ) * np . cos ( longitude ) # Flatten and stack points = np . stack ([ x . ravel (), y . ravel (), z . ravel ()], axis = 1 ) colors = image . reshape ( - 1 , 3 ) # Subsample if too many points if len ( points ) > max_points : idx = np . random . choice ( len ( points ), max_points , replace = False ) points = points [ idx ] colors = colors [ idx ] # Filter out invalid points (very far or very close) valid = ( points [:, 2 ] > 0.1 ) & ( points [:, 2 ] < 100 ) points = points [ valid ] colors = colors [ valid ] return points , colors def save_ply ( filename : str , points : np . ndarray , colors : np . ndarray ): \"\"\"Save point cloud to PLY file.\"\"\" with open ( filename , 'w' ) as f : f . write ( \"ply \\n \" ) f . write ( \"format ascii 1.0 \\n \" ) f . write ( f \"element vertex { len ( points ) } \\n \" ) f . write ( \"property float x \\n \" ) f . write ( \"property float y \\n \" ) f . write ( \"property float z \\n \" ) f . write ( \"property uchar red \\n \" ) f . write ( \"property uchar green \\n \" ) f . write ( \"property uchar blue \\n \" ) f . write ( \"end_header \\n \" ) for ( x , y , z ), ( r , g , b ) in zip ( points , colors ): f . write ( f \" { x : .4f } { y : .4f } { z : .4f } { int ( r ) } { int ( g ) } { int ( b ) } \\n \" ) if __name__ == \"__main__\" : main ()","title":"panorama_depth.py"},{"location":"examples/#batch_inferencepy","text":"Batch process multiple images from the command line. #!/usr/bin/env python3 \"\"\" Batch processing example. Process multiple images from a directory and save depth maps. \"\"\" import argparse from pathlib import Path import cv2 import numpy as np from PIL import Image from tqdm import tqdm import panodac def main (): parser = argparse . ArgumentParser ( description = \"Batch depth prediction\" ) parser . add_argument ( \"input_dir\" , type = str , help = \"Directory with input images\" ) parser . add_argument ( \"output_dir\" , type = str , help = \"Directory for output depth maps\" ) parser . add_argument ( \"--model\" , type = str , default = \"outdoor-resnet101\" , choices = panodac . list_models (), help = \"Model to use\" ) parser . add_argument ( \"--format\" , type = str , default = \"png\" , choices = [ \"png\" , \"npy\" , \"exr\" ], help = \"Output format for depth maps\" ) args = parser . parse_args () # Create output directory input_dir = Path ( args . input_dir ) output_dir = Path ( args . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) # Find all images extensions = { \".jpg\" , \".jpeg\" , \".png\" , \".webp\" , \".bmp\" } image_files = [ f for f in input_dir . iterdir () if f . suffix . lower () in extensions ] if not image_files : print ( f \"No images found in { input_dir } \" ) return print ( f \"Found { len ( image_files ) } images\" ) print ( f \"Using model: { args . model } \" ) print ( f \"Using device: { panodac . get_device () } \" ) # Process images for img_path in tqdm ( image_files , desc = \"Processing\" ): try : # Predict depth depth = panodac . predict ( str ( img_path ), model = args . model ) # Save output output_name = img_path . stem + \"_depth\" if args . format == \"png\" : # Save as 16-bit PNG (depth in mm) depth_mm = ( depth * 1000 ) . astype ( np . uint16 ) output_path = output_dir / f \" { output_name } .png\" cv2 . imwrite ( str ( output_path ), depth_mm ) elif args . format == \"npy\" : # Save as numpy array (original float values in meters) output_path = output_dir / f \" { output_name } .npy\" np . save ( output_path , depth ) elif args . format == \"exr\" : # Save as OpenEXR (requires opencv-python-headless[contrib]) output_path = output_dir / f \" { output_name } .exr\" cv2 . imwrite ( str ( output_path ), depth . astype ( np . float32 )) # Also save a visualization save_visualization ( depth , output_dir / f \" { output_name } _vis.jpg\" ) except Exception as e : print ( f \" \\n Error processing { img_path . name } : { e } \" ) print ( f \" \\n Results saved to { output_dir } \" ) def save_visualization ( depth : np . ndarray , output_path : Path ): \"\"\"Save a colorized depth visualization.\"\"\" # Normalize depth depth_norm = ( depth - depth . min ()) / ( depth . max () - depth . min () + 1e-8 ) depth_vis = ( depth_norm * 255 ) . astype ( np . uint8 ) # Apply colormap depth_colored = cv2 . applyColorMap ( depth_vis , cv2 . COLORMAP_TURBO ) cv2 . imwrite ( str ( output_path ), depth_colored ) if __name__ == \"__main__\" : main ()","title":"batch_inference.py"},{"location":"api/","text":"API Reference # Top-Level API # The main entry points are in the panodac module. predict # predict ( image : Union [ str , Path , ndarray , Image ], model : str = 'outdoor-resnet101' , device : Union [ str , None ] = None ) -> np . ndarray Predict metric depth from any camera image. Supports perspective, fisheye, and 360\u00b0 panorama images. Models are automatically downloaded from HuggingFace on first use. Parameters: Name Type Description Default image Union [ str , Path , ndarray , Image ] Input image. Can be: - Path to image file (str or Path) - numpy array (H, W, 3) in RGB format, values 0-255 - PIL Image required model str Model to use. One of: - 'outdoor-resnet101' (default): Fast outdoor model - 'outdoor-swinl': High-quality outdoor model - 'indoor-resnet101': Fast indoor model - 'indoor-swinl': High-quality indoor model 'outdoor-resnet101' device Union [ str , None] Device to use ('cuda', 'mps', 'cpu', or None for auto-detect) None Returns: Type Description ndarray Depth map as numpy array (H, W) with metric depth in meters. Example import panodac depth = panodac.predict(\"photo.jpg\") depth = panodac.predict(\"panorama.jpg\", model=\"outdoor-swinl\") Source code in src/panodac/__init__.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def predict ( image : Union [ str , Path , np . ndarray , Image . Image ], model : str = \"outdoor-resnet101\" , device : Union [ str , None ] = None , ) -> np . ndarray : \"\"\"Predict metric depth from any camera image. Supports perspective, fisheye, and 360\u00b0 panorama images. Models are automatically downloaded from HuggingFace on first use. Args: image: Input image. Can be: - Path to image file (str or Path) - numpy array (H, W, 3) in RGB format, values 0-255 - PIL Image model: Model to use. One of: - 'outdoor-resnet101' (default): Fast outdoor model - 'outdoor-swinl': High-quality outdoor model - 'indoor-resnet101': Fast indoor model - 'indoor-swinl': High-quality indoor model device: Device to use ('cuda', 'mps', 'cpu', or None for auto-detect) Returns: Depth map as numpy array (H, W) with metric depth in meters. Example: >>> import panodac >>> depth = panodac.predict(\"photo.jpg\") >>> depth = panodac.predict(\"panorama.jpg\", model=\"outdoor-swinl\") \"\"\" if model not in AVAILABLE_MODELS : raise ValueError ( f \"Unknown model ' { model } '. Available models: { AVAILABLE_MODELS } \" ) # Get or create cached predictor cache_key = f \" { model } : { device } \" if cache_key not in _predictor_cache : _predictor_cache [ cache_key ] = DepthPredictor ( model = model , device = device ) predictor = _predictor_cache [ cache_key ] return predictor ( image ) list_models # list_models () -> list [ str ] List all available pretrained models. Returns: Type Description list [ str ] List of model names that can be passed to predict() Source code in src/panodac/__init__.py 33 34 35 36 37 38 39 def list_models () -> list [ str ]: \"\"\"List all available pretrained models. Returns: List of model names that can be passed to predict() \"\"\" return AVAILABLE_MODELS . copy () get_device # get_device ( device : Union [ str , None ] = None ) -> torch . device Get the best available device for inference. Parameters: Name Type Description Default device Union [ str , None] Specific device to use ('cuda', 'mps', 'cpu'), or None for auto-detection. None Returns: Type Description device torch.device for inference Source code in src/panodac/utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_device ( device : Union [ str , None ] = None ) -> torch . device : \"\"\"Get the best available device for inference. Args: device: Specific device to use ('cuda', 'mps', 'cpu'), or None for auto-detection. Returns: torch.device for inference \"\"\" if device is not None : return torch . device ( device ) # Auto-detect best device if torch . backends . mps . is_available (): return torch . device ( \"mps\" ) elif torch . cuda . is_available (): return torch . device ( \"cuda\" ) return torch . device ( \"cpu\" ) Module Structure # panodac/ \u251c\u2500\u2500 __init__.py # Top-level API (predict, list_models, get_device) \u251c\u2500\u2500 predictor.py # DepthPredictor class \u251c\u2500\u2500 hub.py # HuggingFace model download \u251c\u2500\u2500 utils.py # Device detection, image loading \u2514\u2500\u2500 models/ # Neural network architectures \u251c\u2500\u2500 idisc.py # IDisc model (perspective) \u251c\u2500\u2500 idisc_erp.py # IDiscERP model (panorama) \u251c\u2500\u2500 encoder.py # Image encoder \u2514\u2500\u2500 backbones/ # ResNet, Swin Transformer Submodules # Predictor - DepthPredictor class for advanced usage Hub - Model download utilities","title":"Overview"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#top-level-api","text":"The main entry points are in the panodac module.","title":"Top-Level API"},{"location":"api/#panodac.predict","text":"predict ( image : Union [ str , Path , ndarray , Image ], model : str = 'outdoor-resnet101' , device : Union [ str , None ] = None ) -> np . ndarray Predict metric depth from any camera image. Supports perspective, fisheye, and 360\u00b0 panorama images. Models are automatically downloaded from HuggingFace on first use. Parameters: Name Type Description Default image Union [ str , Path , ndarray , Image ] Input image. Can be: - Path to image file (str or Path) - numpy array (H, W, 3) in RGB format, values 0-255 - PIL Image required model str Model to use. One of: - 'outdoor-resnet101' (default): Fast outdoor model - 'outdoor-swinl': High-quality outdoor model - 'indoor-resnet101': Fast indoor model - 'indoor-swinl': High-quality indoor model 'outdoor-resnet101' device Union [ str , None] Device to use ('cuda', 'mps', 'cpu', or None for auto-detect) None Returns: Type Description ndarray Depth map as numpy array (H, W) with metric depth in meters. Example import panodac depth = panodac.predict(\"photo.jpg\") depth = panodac.predict(\"panorama.jpg\", model=\"outdoor-swinl\") Source code in src/panodac/__init__.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def predict ( image : Union [ str , Path , np . ndarray , Image . Image ], model : str = \"outdoor-resnet101\" , device : Union [ str , None ] = None , ) -> np . ndarray : \"\"\"Predict metric depth from any camera image. Supports perspective, fisheye, and 360\u00b0 panorama images. Models are automatically downloaded from HuggingFace on first use. Args: image: Input image. Can be: - Path to image file (str or Path) - numpy array (H, W, 3) in RGB format, values 0-255 - PIL Image model: Model to use. One of: - 'outdoor-resnet101' (default): Fast outdoor model - 'outdoor-swinl': High-quality outdoor model - 'indoor-resnet101': Fast indoor model - 'indoor-swinl': High-quality indoor model device: Device to use ('cuda', 'mps', 'cpu', or None for auto-detect) Returns: Depth map as numpy array (H, W) with metric depth in meters. Example: >>> import panodac >>> depth = panodac.predict(\"photo.jpg\") >>> depth = panodac.predict(\"panorama.jpg\", model=\"outdoor-swinl\") \"\"\" if model not in AVAILABLE_MODELS : raise ValueError ( f \"Unknown model ' { model } '. Available models: { AVAILABLE_MODELS } \" ) # Get or create cached predictor cache_key = f \" { model } : { device } \" if cache_key not in _predictor_cache : _predictor_cache [ cache_key ] = DepthPredictor ( model = model , device = device ) predictor = _predictor_cache [ cache_key ] return predictor ( image )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;predict"},{"location":"api/#panodac.list_models","text":"list_models () -> list [ str ] List all available pretrained models. Returns: Type Description list [ str ] List of model names that can be passed to predict() Source code in src/panodac/__init__.py 33 34 35 36 37 38 39 def list_models () -> list [ str ]: \"\"\"List all available pretrained models. Returns: List of model names that can be passed to predict() \"\"\" return AVAILABLE_MODELS . copy ()","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;list_models"},{"location":"api/#panodac.get_device","text":"get_device ( device : Union [ str , None ] = None ) -> torch . device Get the best available device for inference. Parameters: Name Type Description Default device Union [ str , None] Specific device to use ('cuda', 'mps', 'cpu'), or None for auto-detection. None Returns: Type Description device torch.device for inference Source code in src/panodac/utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_device ( device : Union [ str , None ] = None ) -> torch . device : \"\"\"Get the best available device for inference. Args: device: Specific device to use ('cuda', 'mps', 'cpu'), or None for auto-detection. Returns: torch.device for inference \"\"\" if device is not None : return torch . device ( device ) # Auto-detect best device if torch . backends . mps . is_available (): return torch . device ( \"mps\" ) elif torch . cuda . is_available (): return torch . device ( \"cuda\" ) return torch . device ( \"cpu\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;get_device"},{"location":"api/#module-structure","text":"panodac/ \u251c\u2500\u2500 __init__.py # Top-level API (predict, list_models, get_device) \u251c\u2500\u2500 predictor.py # DepthPredictor class \u251c\u2500\u2500 hub.py # HuggingFace model download \u251c\u2500\u2500 utils.py # Device detection, image loading \u2514\u2500\u2500 models/ # Neural network architectures \u251c\u2500\u2500 idisc.py # IDisc model (perspective) \u251c\u2500\u2500 idisc_erp.py # IDiscERP model (panorama) \u251c\u2500\u2500 encoder.py # Image encoder \u2514\u2500\u2500 backbones/ # ResNet, Swin Transformer","title":"Module Structure"},{"location":"api/#submodules","text":"Predictor - DepthPredictor class for advanced usage Hub - Model download utilities","title":"Submodules"},{"location":"api/hub/","text":"Hub # Utilities for downloading models from HuggingFace Hub. Models are cached locally after first download (~500MB each). download_model # download_model # download_model ( name : str ) -> Tuple [ Path , Path ] Download model config and weights from HuggingFace. Files are cached locally after first download. Parameters: Name Type Description Default name str Model name (e.g., 'outdoor-resnet101') required Returns: Type Description Tuple [ Path , Path ] Tuple of (config_path, weights_path) Raises: Type Description ValueError If model name is not recognized Source code in src/panodac/hub.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def download_model ( name : str ) -> Tuple [ Path , Path ]: \"\"\"Download model config and weights from HuggingFace. Files are cached locally after first download. Args: name: Model name (e.g., 'outdoor-resnet101') Returns: Tuple of (config_path, weights_path) Raises: ValueError: If model name is not recognized \"\"\" if name not in MODELS : raise ValueError ( f \"Unknown model ' { name } '. Available: { list ( MODELS . keys ()) } \" ) model_info = MODELS [ name ] config_path = hf_hub_download ( repo_id = HF_REPO , filename = model_info [ \"config\" ], ) weights_path = hf_hub_download ( repo_id = HF_REPO , filename = model_info [ \"weights\" ], ) return Path ( config_path ), Path ( weights_path ) load_config # load_config # load_config ( config_path : Path ) -> dict Load model configuration from JSON file. Parameters: Name Type Description Default config_path Path Path to config JSON file required Returns: Type Description dict Configuration dictionary Source code in src/panodac/hub.py 64 65 66 67 68 69 70 71 72 73 74 def load_config ( config_path : Path ) -> dict : \"\"\"Load model configuration from JSON file. Args: config_path: Path to config JSON file Returns: Configuration dictionary \"\"\" with open ( config_path , \"r\" ) as f : return json . load ( f ) Available Models # Models are hosted at huggingface.co/yuliangguo/depth-any-camera . Model Config File Weights File outdoor-resnet101 dac_resnet101_outdoor.json dac_resnet101_outdoor.pt outdoor-swinl dac_swinl_outdoor.json dac_swinl_outdoor.pt indoor-resnet101 dac_resnet101_indoor.json dac_resnet101_indoor.pt indoor-swinl dac_swinl_indoor.json dac_swinl_indoor.pt","title":"Hub"},{"location":"api/hub/#hub","text":"Utilities for downloading models from HuggingFace Hub. Models are cached locally after first download (~500MB each).","title":"Hub"},{"location":"api/hub/#download_model","text":"","title":"download_model"},{"location":"api/hub/#panodac.hub.download_model","text":"download_model ( name : str ) -> Tuple [ Path , Path ] Download model config and weights from HuggingFace. Files are cached locally after first download. Parameters: Name Type Description Default name str Model name (e.g., 'outdoor-resnet101') required Returns: Type Description Tuple [ Path , Path ] Tuple of (config_path, weights_path) Raises: Type Description ValueError If model name is not recognized Source code in src/panodac/hub.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def download_model ( name : str ) -> Tuple [ Path , Path ]: \"\"\"Download model config and weights from HuggingFace. Files are cached locally after first download. Args: name: Model name (e.g., 'outdoor-resnet101') Returns: Tuple of (config_path, weights_path) Raises: ValueError: If model name is not recognized \"\"\" if name not in MODELS : raise ValueError ( f \"Unknown model ' { name } '. Available: { list ( MODELS . keys ()) } \" ) model_info = MODELS [ name ] config_path = hf_hub_download ( repo_id = HF_REPO , filename = model_info [ \"config\" ], ) weights_path = hf_hub_download ( repo_id = HF_REPO , filename = model_info [ \"weights\" ], ) return Path ( config_path ), Path ( weights_path )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;download_model"},{"location":"api/hub/#load_config","text":"","title":"load_config"},{"location":"api/hub/#panodac.hub.load_config","text":"load_config ( config_path : Path ) -> dict Load model configuration from JSON file. Parameters: Name Type Description Default config_path Path Path to config JSON file required Returns: Type Description dict Configuration dictionary Source code in src/panodac/hub.py 64 65 66 67 68 69 70 71 72 73 74 def load_config ( config_path : Path ) -> dict : \"\"\"Load model configuration from JSON file. Args: config_path: Path to config JSON file Returns: Configuration dictionary \"\"\" with open ( config_path , \"r\" ) as f : return json . load ( f )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_config"},{"location":"api/hub/#available-models","text":"Models are hosted at huggingface.co/yuliangguo/depth-any-camera . Model Config File Weights File outdoor-resnet101 dac_resnet101_outdoor.json dac_resnet101_outdoor.pt outdoor-swinl dac_swinl_outdoor.json dac_swinl_outdoor.pt indoor-resnet101 dac_resnet101_indoor.json dac_resnet101_indoor.pt indoor-swinl dac_swinl_indoor.json dac_swinl_indoor.pt","title":"Available Models"},{"location":"api/predictor/","text":"Predictor # The DepthPredictor class provides fine-grained control over depth prediction. For most use cases, the top-level panodac.predict() function is sufficient. Use DepthPredictor directly when you need to: Reuse a loaded model across multiple predictions Access the underlying PyTorch model Customize preprocessing behavior DepthPredictor # DepthPredictor # DepthPredictor ( model : str = 'outdoor-resnet101' , device : Union [ str , None ] = None ) High-level interface for depth prediction. Handles model loading, preprocessing, and inference for any camera type. Parameters: Name Type Description Default model str Model name ('outdoor-resnet101', 'outdoor-swinl', etc.) 'outdoor-resnet101' device Union [ str , None] Device to use ('cuda', 'mps', 'cpu', or None for auto) None Source code in src/panodac/predictor.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , model : str = \"outdoor-resnet101\" , device : Union [ str , None ] = None ): self . model_name = model self . device = get_device ( device ) # Download model files config_path , weights_path = download_model ( model ) self . config = load_config ( config_path ) # Build and load model self . _model = self . _build_model () self . _model . load_pretrained ( str ( weights_path )) self . _model . to ( self . device ) self . _model . eval () # Get canonical size from config self . cano_sz = self . config [ \"model\" ] . get ( \"cano_sz\" , [ 1400 , 1400 ]) self . img_size = self . config [ \"model\" ][ \"pixel_encoder\" ][ \"img_size\" ] __call__ # __call__ ( image : Union [ str , Path , ndarray , Image ]) -> np . ndarray Predict depth from an image. Parameters: Name Type Description Default image Union [ str , Path , ndarray , Image ] Input image (path, numpy array, or PIL Image) required Returns: Type Description ndarray Depth map as numpy array (H, W) in meters Source code in src/panodac/predictor.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self , image : Union [ str , Path , np . ndarray , Image . Image ], ) -> np . ndarray : \"\"\"Predict depth from an image. Args: image: Input image (path, numpy array, or PIL Image) Returns: Depth map as numpy array (H, W) in meters \"\"\" # Load image img_np = load_image ( image ) original_h , original_w = img_np . shape [: 2 ] # Check if panorama and use appropriate processing if is_panorama ( img_np ): return self . _predict_panorama ( img_np ) else : return self . _predict_perspective ( img_np ) Usage # from panodac import DepthPredictor # Create predictor (model loads once) predictor = DepthPredictor ( model = \"outdoor-swinl\" , device = \"cuda\" ) # Reuse for multiple images for image_path in image_paths : depth = predictor ( image_path ) The top-level panodac.predict() function caches predictors internally, so this is mainly useful when you need direct access to the predictor instance.","title":"Predictor"},{"location":"api/predictor/#predictor","text":"The DepthPredictor class provides fine-grained control over depth prediction. For most use cases, the top-level panodac.predict() function is sufficient. Use DepthPredictor directly when you need to: Reuse a loaded model across multiple predictions Access the underlying PyTorch model Customize preprocessing behavior","title":"Predictor"},{"location":"api/predictor/#depthpredictor","text":"","title":"DepthPredictor"},{"location":"api/predictor/#panodac.predictor.DepthPredictor","text":"DepthPredictor ( model : str = 'outdoor-resnet101' , device : Union [ str , None ] = None ) High-level interface for depth prediction. Handles model loading, preprocessing, and inference for any camera type. Parameters: Name Type Description Default model str Model name ('outdoor-resnet101', 'outdoor-swinl', etc.) 'outdoor-resnet101' device Union [ str , None] Device to use ('cuda', 'mps', 'cpu', or None for auto) None Source code in src/panodac/predictor.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , model : str = \"outdoor-resnet101\" , device : Union [ str , None ] = None ): self . model_name = model self . device = get_device ( device ) # Download model files config_path , weights_path = download_model ( model ) self . config = load_config ( config_path ) # Build and load model self . _model = self . _build_model () self . _model . load_pretrained ( str ( weights_path )) self . _model . to ( self . device ) self . _model . eval () # Get canonical size from config self . cano_sz = self . config [ \"model\" ] . get ( \"cano_sz\" , [ 1400 , 1400 ]) self . img_size = self . config [ \"model\" ][ \"pixel_encoder\" ][ \"img_size\" ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;DepthPredictor"},{"location":"api/predictor/#panodac.predictor.DepthPredictor.__call__","text":"__call__ ( image : Union [ str , Path , ndarray , Image ]) -> np . ndarray Predict depth from an image. Parameters: Name Type Description Default image Union [ str , Path , ndarray , Image ] Input image (path, numpy array, or PIL Image) required Returns: Type Description ndarray Depth map as numpy array (H, W) in meters Source code in src/panodac/predictor.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __call__ ( self , image : Union [ str , Path , np . ndarray , Image . Image ], ) -> np . ndarray : \"\"\"Predict depth from an image. Args: image: Input image (path, numpy array, or PIL Image) Returns: Depth map as numpy array (H, W) in meters \"\"\" # Load image img_np = load_image ( image ) original_h , original_w = img_np . shape [: 2 ] # Check if panorama and use appropriate processing if is_panorama ( img_np ): return self . _predict_panorama ( img_np ) else : return self . _predict_perspective ( img_np )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;__call__"},{"location":"api/predictor/#usage","text":"from panodac import DepthPredictor # Create predictor (model loads once) predictor = DepthPredictor ( model = \"outdoor-swinl\" , device = \"cuda\" ) # Reuse for multiple images for image_path in image_paths : depth = predictor ( image_path ) The top-level panodac.predict() function caches predictors internally, so this is mainly useful when you need direct access to the predictor instance.","title":"Usage"}]}